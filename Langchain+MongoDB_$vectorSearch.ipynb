{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPrQWP+2Qd7/TnMraJydhN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prakul/MongoDB-AI-Resources/blob/main/Langchain%2BMongoDB_%24vectorSearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install prerequisites dependencies\n"
      ],
      "metadata": {
        "id": "3CaG54pnNhqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain pypdf pymongo openai python-dotenv tiktoken"
      ],
      "metadata": {
        "id": "HwQZnho4m5IY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup the environment"
      ],
      "metadata": {
        "id": "H0zbGavtNpMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from pymongo import MongoClient\n",
        "\n",
        "load_dotenv(override=True)\n",
        "\n",
        "# Add an environment file to the notebook root directory called .env with MONGO_URI=\"xxx\" to load these envvars\n",
        "\n",
        "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
        "MONGO_URI = os.environ[\"MONGO_URI\"]\n",
        "DB_NAME = \"langchain-test-2\"\n",
        "COLLECTION_NAME = \"test\"\n",
        "ATLAS_VECTOR_SEARCH_INDEX_NAME = \"default\"\n",
        "EMBEDDING_FIELD_NAME = \"embedding\"\n",
        "client = MongoClient(MONGO_URI)\n",
        "db = client[DB_NAME]\n",
        "MONGODB_COLLECTION = db[COLLECTION_NAME]"
      ],
      "metadata": {
        "id": "JWl8u9gdyBGR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "68291f70-0f06-4617-f372-5d238856042f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e7ea905c74c3>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdotenv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpymongo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMongoClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## INSERT DATA"
      ],
      "metadata": {
        "id": "_NbBxI940_yK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"https://arxiv.org/pdf/2303.08774.pdf\")\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "bLosUpvDmDOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0)\n",
        "docs = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "Zot6nbm1QLUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g20i__W169jU",
        "outputId": "2a35db50-908a-4acb-cd0d-d73558181a70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='GPT-4 Technical Report\\nOpenAIâˆ—\\nAbstract\\nWe report the development of GPT-4, a large-scale, multimodal model which can\\naccept image and text inputs and produce text outputs. While less capable than\\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance\\non various professional and academic benchmarks, including passing a simulated\\nbar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-', metadata={'source': '/tmp/tmp01sv5lz3/tmp.pdf', 'page': 0})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import MongoDBAtlasVectorSearch\n",
        "\n",
        "# insert the documents in MongoDB Atlas Vector Search\n",
        "x = MongoDBAtlasVectorSearch.from_documents(\n",
        "     documents=docs, embedding=OpenAIEmbeddings(disallowed_special=()), collection=MONGODB_COLLECTION, index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME\n",
        " )\n"
      ],
      "metadata": {
        "id": "NUrRHjYumSh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CREATE INDEX\n",
        "\n",
        " Create an Atlas search index via Atlas UI -> Search -> JSON Editor with the following definition\n",
        " https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/\n",
        " ```\n",
        "  {\n",
        "   \"mappings\": {\n",
        "     \"dynamic\": true,\n",
        "     \"fields\": {\n",
        "       \"embedding\": {\n",
        "         \"dimensions\": 1536,\n",
        "         \"similarity\": \"cosine\",\n",
        "         \"type\": \"knnVector\"\n",
        "       }\n",
        "     }\n",
        "   }\n",
        " }\n",
        "```"
      ],
      "metadata": {
        "id": "F7v5H-48MW18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "{\n",
        "  \"mappings\": {\n",
        "    \"dynamic\": true,\n",
        "    \"fields\": {\n",
        "      \"embedding\": {\n",
        "        \"dimensions\": 1536,\n",
        "        \"similarity\": \"cosine\",\n",
        "        \"type\": \"knnVector\"\n",
        "      },\n",
        "      \"source\": [\n",
        "        {\n",
        "          \"normalizer\": \"lowercase\",\n",
        "          \"type\": \"token\"\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "dCnjAd-_0oB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA QUERY"
      ],
      "metadata": {
        "id": "I0yiKlx9MRXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pymongo\n",
        "\n",
        "client = pymongo.MongoClient(MONGO_URI)\n",
        "db = client[\"langchain-test-2\"]\n",
        "collection = db[\"test\"]"
      ],
      "metadata": {
        "id": "VXNk-DBXOqvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai"
      ],
      "metadata": {
        "id": "0Knl0b5KPKZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "model = \"text-embedding-ada-002\"\n",
        "def get_embedding(text: str) -> list[float]:\n",
        "    return openai.Embedding.create(input=[text], model=model)[\"data\"][0][\"embedding\"]\n"
      ],
      "metadata": {
        "id": "PtxzKgGHPDFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $VECTORSEARCH MQL Query without Filter"
      ],
      "metadata": {
        "id": "FvyjZlzs8b8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"gpt-4\"\n",
        "results = collection.aggregate([\n",
        "{\n",
        "\"$vectorSearch\": {\n",
        "\"index\": \"default\",\n",
        "\"queryVector\": get_embedding(query),\n",
        "\"numCandidates\": 200,\n",
        "\"limit\": 20,\n",
        "\"path\": \"embedding\"\n",
        "}},\n",
        " {\n",
        "    \"$project\": {\n",
        "      \"_id\": 0,\n",
        "      \"source\": 1,\n",
        "      \"score\": { \"$meta\": \"vectorSearchScore\" }\n",
        "    }\n",
        "}\n",
        "\n",
        "])\n",
        "\n",
        "for document in results:\n",
        "    print(document)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b02da62c-4f26-46f5-d2d6-30df974acdf8",
        "id": "nRQPHzWn6i9w"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'source': '/tmp/tmpevpslauk/tmp.pdf', 'score': 0.9324517250061035}\n",
            "{'source': '/tmp/tmpevpslauk/tmp.pdf', 'score': 0.9298850297927856}\n",
            "{'source': '/tmp/tmpevpslauk/tmp.pdf', 'score': 0.9282032251358032}\n",
            "{'source': '/tmp/tmpevpslauk/tmp.pdf', 'score': 0.9266623854637146}\n",
            "{'source': '/tmp/tmpevpslauk/tmp.pdf', 'score': 0.9254909157752991}\n",
            "{'source': '/tmp/tmpevpslauk/tmp.pdf', 'score': 0.9251236915588379}\n",
            "{'source': '/tmp/tmpevpslauk/tmp.pdf', 'score': 0.9243311285972595}\n",
            "{'source': '/tmp/tmpevpslauk/tmp.pdf', 'score': 0.9239716529846191}\n",
            "{'source': '/tmp/tmpevpslauk/tmp.pdf', 'score': 0.9236046075820923}\n",
            "{'source': '/tmp/tmpevpslauk/tmp.pdf', 'score': 0.9231781959533691}\n",
            "{'source': '/tmp/tmpevpslauk/tmp.pdf', 'score': 0.9222846031188965}\n",
            "{'source': '/tmp/tmpevpslauk/tmp.pdf', 'score': 0.9207495450973511}\n",
            "{'source': '/tmp/tmpevpslauk/tmp.pdf', 'score': 0.9207268357276917}\n",
            "{'source': '/tmp/tmpevpslauk/tmp.pdf', 'score': 0.9202592968940735}\n",
            "{'source': '/tmp/tmpevpslauk/tmp.pdf', 'score': 0.9199975728988647}\n",
            "{'source': '/tmp/tmpevpslauk/tmp.pdf', 'score': 0.9198940396308899}\n",
            "{'source': '/tmp/tmpevpslauk/tmp1.pdf', 'score': 0.9191571474075317}\n",
            "{'source': '/tmp/tmpevpslauk/tmp.pdf', 'score': 0.9190821647644043}\n",
            "{'source': '/tmp/tmpevpslauk/tmp.pdf', 'score': 0.9185327291488647}\n",
            "{'source': '/tmp/tmpevpslauk/tmp.pdf', 'score': 0.9182653427124023}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $VECTORSEARCH MQL Query with Filter"
      ],
      "metadata": {
        "id": "gju0QbKrv_Gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"gpt-4\"\n",
        "results = collection.aggregate([\n",
        "{\n",
        "\"$vectorSearch\": {\n",
        "    \"index\": \"default\",\n",
        "    \"queryVector\": get_embedding(query),\n",
        "    \"limit\": 2,\n",
        "    \"numCandidates\": 200,\n",
        "    \"path\": \"embedding\",\n",
        "    \"filter\": {\n",
        "        \"source\": { \"$eq\": \"/tmp/tmpevpslauk/tmp1.pdf\"}\n",
        "    }\n",
        " }},\n",
        " {\n",
        "    \"$project\": {\n",
        "      \"_id\": 0,\n",
        "      \"source\": 1,\n",
        "      \"score\": { \"$meta\": \"vectorSearchScore\" }\n",
        "    }\n",
        "}\n",
        "\n",
        "])\n",
        "\n",
        "for document in results:\n",
        "    print(document)\n"
      ],
      "metadata": {
        "id": "Nya4PZ1dPW-N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfd74e56-4ce8-4ba0-cc0f-454461fb6428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'source': '/tmp/tmpevpslauk/tmp1.pdf', 'score': 0.9191571474075317}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langchain semantic search without filter"
      ],
      "metadata": {
        "id": "VIG9jkYmw6nE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import MongoDBAtlasVectorSearch\n",
        "\n",
        "vector_search = MongoDBAtlasVectorSearch.from_connection_string(\n",
        "    MONGO_URI,\n",
        "    DB_NAME + \".\" + COLLECTION_NAME,\n",
        "    OpenAIEmbeddings(disallowed_special=()),\n",
        "    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME\n",
        ")\n"
      ],
      "metadata": {
        "id": "TZp7_CBfxTOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"gpt-4\"\n",
        "results = vector_search.similarity_search(\n",
        "    query=query,\n",
        "    k=20,\n",
        ")\n",
        "\n",
        "# Display results\n",
        "#print(dict(results[0].metadata).keys())\n",
        "for result in results:\n",
        "    print( result)\n"
      ],
      "metadata": {
        "id": "XqwOCCndyID3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langchain semantic search with filter"
      ],
      "metadata": {
        "id": "DycPmA7CzPpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"gpt-4\"\n",
        "results = vector_search.similarity_search(\n",
        "    query=query,\n",
        "    k=20,\n",
        "    pre_filter=\n",
        "            {\n",
        "        \"source\": { \"$eq\": \"/tmp/tmpevpslauk/tmp1.pdf\"}\n",
        "    },\n",
        ")\n",
        "\n",
        "# Display results\n",
        "#print(dict(results[0].metadata).keys())\n",
        "for result in results:\n",
        "    print( dict(result.metadata)[\"source\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNUBe6OYzPAL",
        "outputId": "6359a16d-9994-441b-dc8d-4ea17f07b998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/tmpevpslauk/tmp1.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------------------------------\n"
      ],
      "metadata": {
        "id": "n7GvXEb6z1Ge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langchain QA without filters"
      ],
      "metadata": {
        "id": "-_6mfQ8Lzx2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_retriever = vector_search.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\n",
        "        \"k\": 200,\n",
        "        \"post_filter_pipeline\": [{\"$limit\": 25}]\n",
        "    }\n",
        ")\n"
      ],
      "metadata": {
        "id": "qPBUH9HGF60e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")"
      ],
      "metadata": {
        "id": "_QrjpVtMaw97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(llm=OpenAI(),chain_type=\"stuff\", retriever=qa_retriever, return_source_documents=True, chain_type_kwargs={\"prompt\": PROMPT})\n",
        "\n",
        "docs = qa({\"query\": \"gpt-4 compute requirements\"})\n",
        "\n",
        "print(docs[\"result\"])\n",
        "print(docs['source_documents'])"
      ],
      "metadata": {
        "id": "ExGokn9WbHJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langchain QA with filters"
      ],
      "metadata": {
        "id": "E8UyMUJXbCiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_retriever = vector_search.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\n",
        "        \"k\": 20,\n",
        "        \"pre_filter\":\n",
        "            {\n",
        "        \"source\": { \"$eq\": \"/tmp/tmpevpslauk/tmp1.pdf\"}\n",
        "    },\n",
        "\n",
        "        \"post_filter_pipeline\": [{\"$limit\": 2}]\n",
        "    }\n",
        ")\n"
      ],
      "metadata": {
        "id": "1LCYwkE9w_9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(llm=OpenAI(),chain_type=\"stuff\", retriever=qa_retriever, return_source_documents=True, chain_type_kwargs={\"prompt\": PROMPT})\n",
        "\n",
        "docs = qa({\"query\": \"gpt-4 compute requirements\"})\n",
        "\n",
        "print(docs['result'])\n",
        "#print(docs['source_documents'])"
      ],
      "metadata": {
        "id": "IvKNAyBCGbqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langchain QA misc"
      ],
      "metadata": {
        "id": "uOerMAUlbcrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "question = \"How much better is GPT-4 in reducing hallucinations over GPT-3.5\"\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHZQfWaXmV3N",
        "outputId": "8c28a53c-c1e7-42e3-e912-b17030d0cf6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'How much better is GPT-4 in reducing hallucinations over GPT-3.5',\n",
              " 'result': \"I'm sorry, but I don't have access to information about the specific capabilities or improvements of GPT-4 over GPT-3.5. My knowledge is based on GPT-3, and I don't have real-time updates on subsequent versions.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(llm, retriever=vector_search.as_retriever())\n",
        "qa_chain({\"query\": question})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZLYniCQmYhi",
        "outputId": "af7518bd-222e-4cde-eaef-db29310f6969"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'How much better is GPT-4 in reducing hallucinations over GPT-3.5',\n",
              " 'result': 'GPT-4 scores 19 percentage points higher than GPT-3.5 in reducing hallucinations.'}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}